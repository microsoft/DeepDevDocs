# Author Inference Entry Script

This documentation tells you how to write an inference entry script before deploying your model to DeepDev.

Inference entry script is where you tell DeepDev how to load your model, read the user input and get results returned by your model.

An entry script is actually a python module which can be loaded and imported by other python scripts. And the module must contain three components:
1. A global variable named `model`.
2. A function named `run`.
3. A function named `init`.

## `init` Function
`init` function is executed when the inference service get start. It accepts a string parameter `model_dir` which is the path to a folder containing all the model files registered before.

> NOTE: For how to register models, see [Register Model](register-model.md).

Inside `init` function. Model should be loaded into memory and assigned to global variable `model`. Other prerequisite operations, like decompressing dependent files, are recommended to be done here, too.

`init` function should be decorated by `model_init` decorator. It can be imported from package `deepdev.services.deploy`.

Here is an example for `inti` function:
``` python
from deepdev.services.deploy import model_init

@model_init
def init(model_dir: str) -> None:
    global model
    model = fancy_model_loader(model_dir)
```

## `run` Function
`run` function takes user input as arguments, then do pre processing jobs like tokenizing before feeding the data into model. It returns output generated by model.

### Define input and output schema
It is strongly recommended to decorate `run` function with input and output samples. So that a swagger file can be automatically generated to tell users how to call the inference api. Package `inference_schema` helps do that.

`inference_schema` offers two decorators `input_schema` and `output_schema`. It also offers three kinds of sample data wrapper classes. `StandardPythonParameterType` is the most commonly used one among them.

> NOTE: DeepDev follows the same way as Azure Machine Learning to compose the input and output schema. For more information, refer to [Azure Machine Learning's documentation](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-advanced-entry-script#automatically-generate-a-swagger-schema).

Here is an example for `run` function:
```python
from inference_schema.schema_decorators import input_schema, output_schema
from inference_schema.parameter_types.standard_py_parameter_type import (
    StandardPythonParameterType,
)

@input_schema("input_param", StandardPythonParameterType("input sequence.")) # Mandatory
@input_schema(
    "beam_size", StandardPythonParameterType(3), convert_to_provided_type=False
) # Optional
@output_schema(StandardPythonParameterType([
    (98.3, "Output sequence.")
]))
def run(input_param, beam_size=5):
    preprocessed = tokenizer.tokenize(input_param)
    preprocessed = normalize(preprocessed)

    results = model(preprocessed)

    return results
```

In above example, users should call the inference api with JSON-formatted request body:
``` json
{
    "input_param": "input sequence.",
    "beam_size": 3
}
```

And get response like:
``` json
[
    [98.3, "Output sequence."]
]
```

## Testing your script locally

It is highly recommended to include an executable block in inference entry script, encapsulated by `if __name__ == "__main__"`.

While deployment does not require an executable block, having one can help you run the script locally and detect inference issues in advance.

From the `__main__` block, you can call `init` directly with a model_dir, and you can call `run` with sample inputs.

Here is an example of the executable block:

```python
from argparse import ArgumentParser

if __name__ == '__main__':
    parser = ArgumentParser()

    parser.add_argument("--model_dir", type=str, required=True)
    parser.add_argument("--input_param", type=str, required=True)
    parser.add_argument("--beam_size", type=int, default=3)

    args = parser.parse_args()

    init(args.model_dir)

    result = run(args.input_param, args.beam_size)

    print("Model output: " + result)
```
